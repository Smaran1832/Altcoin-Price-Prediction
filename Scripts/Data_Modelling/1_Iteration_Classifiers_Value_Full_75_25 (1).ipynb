{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UAde64jpiSb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateBinaryClassification(predictions, actuals):\n",
        "    list = []\n",
        "    contigency = pd.crosstab(actuals,predictions)\n",
        "    TP = contigency['yes']['yes']\n",
        "    TN = contigency['no']['no']\n",
        "    FP = contigency['yes']['no']\n",
        "    FN = contigency['no']['yes']\n",
        "    n = contigency.sum().sum()\n",
        "\n",
        "    Acuracy = (TP + TN)/n\n",
        "    Recall = TP/(TP+FN)\n",
        "    Precision = TP/(TP+FP)\n",
        "    FScore = 2*Recall*Precision/(Recall+Precision)\n",
        "    list.append(Acuracy)\n",
        "    list.append(Recall)\n",
        "    list.append(Precision)\n",
        "    list.append(FScore)\n",
        "    # return Acuracy, Recall, Precision, FScore\n",
        "    return list"
      ],
      "metadata": {
        "id": "L5exlH8YqvUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluateBinaryClassification(predictions, actuals):\n",
        "#     list = []\n",
        "#     TP = 0\n",
        "#     TN = 0 \n",
        "#     FP = 0 \n",
        "#     FN = 0 \n",
        "#     n = np.size(predictions)\n",
        "#     for i in range(0,n):\n",
        "#       if predictions[i] == 'yes' and actuals[i] == 'yes':\n",
        "#         TP = TP + 1\n",
        "#       elif predictions[i] == 'yes' and actuals[i] == 'no':\n",
        "#         FP = FP + 1\n",
        "#       elif predictions[i] == 'no' and actuals[i] == 'no':\n",
        "#         TN = TN + 1\n",
        "#       elif predictions[i] == 'no' and actuals[i] == 'yes':\n",
        "#         FN = FN + 1\n",
        "\n",
        "\n",
        "#     Acuracy = (TP + TN)/n\n",
        "#     Recall = TP/(TP+FN)\n",
        "#     Precision = TP/(TP+FP)\n",
        "#     FScore = 0\n",
        "#     if Recall+Precision != 0:\n",
        "#       Fscore = 2*Recall*Precision/(Recall+Precision)\n",
        "#     list.append(Acuracy)\n",
        "#     list.append(Recall)\n",
        "#     list.append(Precision)\n",
        "#     list.append(Fscore)\n",
        "#     # return Acuracy, Recall, Precision, FScore\n",
        "#     return list"
      ],
      "metadata": {
        "id": "I9EiuytpyIr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 40\n",
        "scores = []"
      ],
      "metadata": {
        "id": "P7xTI6BuqxA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(r\"/content/2022Binance_Original_LowCloseRemoved_All_Change_num_btc_bin_2day.xlsx\", na_values='?',index_col=0)\n",
        "print(data.dtypes)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "MvF6pI0eq1P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yesno(x):\n",
        "  if x == 1:\n",
        "    return 'yes'\n",
        "  else:\n",
        "    return 'no'"
      ],
      "metadata": {
        "id": "vlS5xF4jrlrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Binance_change'] = data['Binance_change'].apply(yesno)"
      ],
      "metadata": {
        "id": "7gI1_HEtrplY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "6JaWWCtmrr0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data.drop(['Internet_Penetration_USA','Population_USA','GDP_USA','Inflation_USA','Nasdaq_High','Nasdaq_Low','Nasdaq_Close','Nasdaq_Adj Close','S&P500_High','S&P500_Low','S&P500_Close','S&P500_Adj Close','BTC_High','BTC_Low','BTC_Close','BTC_Market_Cap','BTC_Open'],axis=1)"
      ],
      "metadata": {
        "id": "lU0Gfa_H8SQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data= data.drop(['BTC_Change_Bin_1day','Internet_Penetration_USA','Population_USA','GDP_USA','Inflation_USA','Gold_Change_bin_1day','Oil_Change_Bin_1day','Nasdaq_Change_bin_1day','S&P500_Change_Bin_1day','BTC_Volume_Change_bin_1day','BTC_Market_Cap_Bin_1day'], axis=1) #date dropped"
      ],
      "metadata": {
        "id": "YZuOvb2yrsab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "WH1P1VsjSdq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Metrics = ['Accuracy','Recall','Precision','Fscore']\n",
        "\n",
        "compare_df = pd.DataFrame(columns = Metrics)"
      ],
      "metadata": {
        "id": "CF7jPkYAsBHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = data.shape"
      ],
      "metadata": {
        "id": "ep_hcC7Y1WOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = s[0]*0.75 + 1\n",
        "T = int(T)"
      ],
      "metadata": {
        "id": "oliNItRm1d6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNeighbours Classifier"
      ],
      "metadata": {
        "id": "2aZcxPvMvOZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x = np.arange(1,100,1)\n",
        "# for k in x:\n",
        "acc = 0\n",
        "recall = 0\n",
        "prec = 0\n",
        "fscore = 0;\n",
        "\n",
        "for i in range(0,10):\n",
        "    # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "  y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "\n",
        "  # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  model = KNeighborsClassifier(p = 2, n_neighbors=17)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  acc += model.score(X_test, y_test)\n",
        "  recall += recall_score(y_test, y_pred, average='micro')  # average='micro' added to resolve error\n",
        "  prec += precision_score(y_test, y_pred, average='micro')\n",
        "  fscore += f1_score(y_test,y_pred, average='micro')\n",
        "    \n",
        "acc /= 10\n",
        "recall /= 10\n",
        "prec /= 10\n",
        "fscore /= 10\n",
        "\n",
        "compare_df.loc['KNeighbors K=13 75-25'] = [acc,recall,prec,fscore]\n",
        "# print(f\"k={k}\\tacc={acc}\\trecall={recall}\\tprec={prec}\")\n",
        "# print(compare_df)\n",
        "    "
      ],
      "metadata": {
        "id": "u6a550zwCayz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "iePL_kwpwbUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets, linear_model, metrics\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "\n",
        "for i in range(0,10):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "  y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  # create logistic regression object\n",
        "  reg = linear_model.LogisticRegression()\n",
        "    \n",
        "  # train the model using the training sets\n",
        "  reg.fit(X_train, y_train)\n",
        "    \n",
        "  # making predictions on the testing set \n",
        "  y_predict_LR = reg.predict(X_test)\n",
        "  # print(y_predict_LR)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_LR,y_test)\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['LogisticRegression'] = evaluateBinaryClassification(y_predict_LR,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= 10\n",
        "avg_recall /= 10\n",
        "avg_precision /= 10\n",
        "avg_fscore /= 10\n",
        "compare_df.loc['Logistic Regression 75-25'] = [avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "3vdYDdPXEDtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine"
      ],
      "metadata": {
        "id": "9KkDmXgYxcBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "for i in range(0,10):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "  y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  model = svm.SVC(kernel = 'linear', random_state = 0, C=3.0)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_predict_SVM = model.predict(X_test)\n",
        "  # print(y_predict_SVM)\n",
        "  \n",
        "  list = evaluateBinaryClassification(y_predict_SVM,y_test)\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['SVM'] = evaluateBinaryClassification(y_predict_SVM,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= 10\n",
        "avg_recall /= 10\n",
        "avg_precision /= 10\n",
        "avg_fscore /= 10\n",
        "compare_df.loc['SVM 75-25'] = [avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "rmHfBpxoFstE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting"
      ],
      "metadata": {
        "id": "IQFu_n4sfMXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for i in range(0,10):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "  y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  model = GradientBoostingClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_predict_GradientBoostingClassifier = model.predict(X_test)\n",
        "  # print(y_predict_GradientBoostingClassifier)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_GradientBoostingClassifier,y_test)\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['GradientBoostingClassifier'] = evaluateBinaryClassification(y_predict_GradientBoostingClassifier,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= 10\n",
        "avg_recall /= 10\n",
        "avg_precision /= 10\n",
        "avg_fscore /= 10\n",
        "compare_df.loc['Gradient Boosting 75-25'] = [avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "n7wygeIrerSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Discriminant Analysis"
      ],
      "metadata": {
        "id": "aAs_jVDXy005"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "\n",
        "for i in range(0,10):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "  y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  lda = LinearDiscriminantAnalysis()          #not working for n_components=2\n",
        "  X_train = lda.fit_transform(X_train, y_train)\n",
        "  X_test = lda.transform(X_test)\n",
        "\n",
        "  classifier = RandomForestClassifier(max_depth=3, random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "\n",
        "  #plotDecisionTree(classTree, feature_names=X_train.columns, class_names=classTree.classes_)\n",
        "  y_predict_LDA = classifier.predict(X_test)\n",
        "  # print(y_predict_LDA)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_LDA,y_test)\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['LDA'] = evaluateBinaryClassification(y_predict_LDA,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= 10\n",
        "avg_recall /= 10\n",
        "avg_precision /= 10\n",
        "avg_fscore /= 10\n",
        "compare_df.loc['LDA 75-25'] = [avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "89IKwgU4GZ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LGMB Classifier"
      ],
      "metadata": {
        "id": "aJ2FI1y10uCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "\n",
        "for i in range(0,10):\n",
        "\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "  y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  model = LGBMClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_predict_LGMBclassifier = model.predict(X_test)\n",
        "  # print(y_predict_LGMBclassifier)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_LGMBclassifier,y_test)\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['LGBMclassifier'] = evaluateBinaryClassification(y_predict_LGMBclassifier,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= 10\n",
        "avg_recall /= 10\n",
        "avg_precision /= 10\n",
        "avg_fscore /= 10\n",
        "compare_df.loc['LGBMclassifier 75-25'] = [avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "BSiF5wNxHdZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XG BOOST"
      ],
      "metadata": {
        "id": "fsvAfMKffHQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# avg_accuracy = 0\n",
        "# avg_recall = 0\n",
        "# avg_precision = 0\n",
        "# avg_fscore = 0\n",
        "# max_accuracy = 0;\n",
        "# for i in range(0,10):\n",
        "\n",
        "#                # Get the train-test subsets\n",
        "#   X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "#   y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "#         # Scale the X features according to training data \n",
        "#   scaler = StandardScaler()\n",
        "#   X_train = scaler.fit_transform(X_train)\n",
        "#   X_test = scaler.transform(X_test)\n",
        "\n",
        "#   model = XGBClassifier()\n",
        "#   model.fit(X_train, y_train)\n",
        "#   y_predict_XgBoost = model.predict(X_test)\n",
        "#   # print(y_predict_XgBoost)\n",
        "\n",
        "#   list = evaluateBinaryClassification(y_predict_XgBoost,y_test)\n",
        "#   avg_accuracy += list[0]\n",
        "#   avg_recall += list[1]\n",
        "#   avg_precision += list[2]\n",
        "#   avg_fscore += list[3]\n",
        "\n",
        "#   # compare_df.loc['XgBoost'] = evaluateBinaryClassification(y_predict_XgBoost,y_test)\n",
        "#   # print(compare_df)\n",
        "\n",
        "# avg_accuracy /= 10\n",
        "# avg_recall /= 10\n",
        "# avg_precision /= 10\n",
        "# avg_fscore /= 10\n",
        "# compare_df.loc['XgBoost 75-25'] = [avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# # print(compare_df)"
      ],
      "metadata": {
        "id": "aGuE2owWfFV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Classifier"
      ],
      "metadata": {
        "id": "5pxfvVsq1PZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "\n",
        "for i in range(0,10):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[0:T,:-1], data.iloc[T:,:-1]\n",
        "  y_train, y_test = data.iloc[0:T,-1], data.iloc[T:,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "  mlp = MLPClassifier()\n",
        "  mlp.fit(X_train, y_train)\n",
        "  y_predict_mlp = mlp.predict(X_test)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_mlp,y_test)\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  #compare_df = pd.DataFrame()\n",
        "  # compare_df.loc['MLP'] = evaluateBinaryClassification(y_predict_mlp,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= 10\n",
        "avg_recall /= 10\n",
        "avg_precision /= 10\n",
        "avg_fscore /= 10\n",
        "compare_df.loc['MLP 75-25'] = [avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "rXPhs1knIJXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(compare_df)"
      ],
      "metadata": {
        "id": "tGjmGjEt1OjR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}