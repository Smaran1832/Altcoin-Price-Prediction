{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UAde64jpiSb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateBinaryClassification(predictions, actuals):\n",
        "    list = []\n",
        "    contigency = pd.crosstab(actuals,predictions)\n",
        "    TP = contigency['yes']['yes']\n",
        "    TN = contigency['no']['no']\n",
        "    FP = contigency['yes']['no']\n",
        "    FN = contigency['no']['yes']\n",
        "    n = contigency.sum().sum()\n",
        "\n",
        "    Acuracy = (TP + TN)/n\n",
        "    Recall = TP/(TP+FN)\n",
        "    Precision = TP/(TP+FP)\n",
        "    FScore = 2*Recall*Precision/(Recall+Precision)\n",
        "    list.append(Acuracy)\n",
        "    list.append(Recall)\n",
        "    list.append(Precision)\n",
        "    list.append(FScore)\n",
        "    # return Acuracy, Recall, Precision, FScore\n",
        "    return list"
      ],
      "metadata": {
        "id": "L5exlH8YqvUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 40\n",
        "scores = []"
      ],
      "metadata": {
        "id": "P7xTI6BuqxA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(r\"/content/2021XRP_Full_NoScaling_Vader_Binary_Close-Open.xlsx\", na_values='?',index_col=0)\n",
        "print(data.dtypes)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "MvF6pI0eq1P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import collections\n",
        "# # using Counter to find frequency of elements\n",
        "# frequency = collections.Counter(data['S&P500_Binary'])\n",
        "# print(frequency)"
      ],
      "metadata": {
        "id": "X9xuOIaSDDVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = data.reset_index()\n",
        "# import matplotlib.pyplot as plt\n",
        "# data['percentage_change'] = ((data['BTC_Close'] - data['BTC_Open']) / data['BTC_Open']) * 100\n",
        "# plt.plot(data['Date'],data['percentage_change'])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "c7eOG0q_FCvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yesno(x):\n",
        "  if x == 1:\n",
        "    return 'yes'\n",
        "  else:\n",
        "    return 'no'"
      ],
      "metadata": {
        "id": "vlS5xF4jrlrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Ethereum_Change'] = data['Ethereum_Change'].apply(yesno)\n",
        "# data['Stellar_Change'] = data['Stellar_Change'].apply(yesno)\n",
        "# data['Monero_Change'] = data['Monero_Change'].apply(yesno)\n",
        "# data['Litecoin_Change'] = data['Litecoin_Change'].apply(yesno)\n",
        "# data['Tron_Change'] = data['Tron_Change'].apply(yesno)\n",
        "# data['Cardano_Change'] = data['Cardano_Change'].apply(yesno)\n",
        "# data['XRP_Change'] = data['XRP_Change'].apply(yesno)\n",
        "# data['Binance_Change'] = data['Binance_Change'].apply(yesno)\n",
        "# data['BTC_Change'] = data['BTC_Change'].apply(yesno)"
      ],
      "metadata": {
        "id": "7gI1_HEtrplY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "6JaWWCtmrr0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "13wOxeNsoKER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Metrics = ['Max Accuracy','Accuracy','Recall','Precision','Fscore']\n",
        "\n",
        "compare_df = pd.DataFrame(columns = Metrics)"
      ],
      "metadata": {
        "id": "CF7jPkYAsBHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier"
      ],
      "metadata": {
        "id": "8czBCoD9vExn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "  # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "  # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  classTree = DecisionTreeClassifier()\n",
        "  classTree.fit(X_train, y_train)\n",
        "\n",
        "  #plotDecisionTree(classTree, feature_names=X_train.columns, class_names=classTree.classes_)\n",
        "  y_predict_DT = classTree.predict(X_test)\n",
        "  # print(y_predict_DT)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_DT,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "  # compare_df.loc[len(compare_df.index)] = evaluateBinaryClassification(y_predict_DT,y_test)\n",
        "  # print(compare_df)\n",
        "  \n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['Decision Tree'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)\n"
      ],
      "metadata": {
        "id": "PNz1cAYNsYCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNeighbours Classifier"
      ],
      "metadata": {
        "id": "2aZcxPvMvOZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x = np.arange(1,100,1)\n",
        "# for k in x:\n",
        "acc = 0\n",
        "recall = 0\n",
        "prec = 0\n",
        "fscore = 0;\n",
        "max_accuracy = 0;\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "for train_index,test_index in kfold.split(data):\n",
        "    # Get the train-test subsets\n",
        "    X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "    y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "    # Scale the X features according to training data \n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    model = KNeighborsClassifier(p = 1, n_neighbors=13)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    max_accuracy = max(max_accuracy,model.score(X_test, y_test))\n",
        "    acc += model.score(X_test, y_test)\n",
        "    recall += recall_score(y_test, y_pred, average='micro')  # average='micro' added to resolve error\n",
        "    prec += precision_score(y_test, y_pred, average='micro')\n",
        "    fscore += f1_score(y_test,y_pred, average='micro')\n",
        "    \n",
        "acc /= kfold.get_n_splits()\n",
        "recall /= kfold.get_n_splits()\n",
        "prec /= kfold.get_n_splits()\n",
        "fscore /= kfold.get_n_splits()\n",
        "\n",
        "compare_df.loc['KNeighbors K=13'] = [max_accuracy,acc,recall,prec,fscore]\n",
        "# print(f\"k={k}\\tacc={acc}\\trecall={recall}\\tprec={prec}\")\n",
        "# print(compare_df)\n",
        "    "
      ],
      "metadata": {
        "id": "hen-VIFRscMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "iePL_kwpwbUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets, linear_model, metrics\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  # create logistic regression object\n",
        "  reg = linear_model.LogisticRegression(max_iter=200)\n",
        "    \n",
        "  # train the model using the training sets\n",
        "  reg.fit(X_train, y_train)\n",
        "    \n",
        "  # making predictions on the testing set \n",
        "  y_predict_LR = reg.predict(X_test)\n",
        "  # print(y_predict_LR)\n",
        "  \n",
        "  list = evaluateBinaryClassification(y_predict_LR,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['LogisticRegression'] = evaluateBinaryClassification(y_predict_LR,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['Logistic Regression'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "VwUXgRbkwCeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "H3kmDBL2xEfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  nb = GaussianNB()\n",
        "  nb.fit(X_train, y_train)\n",
        "\n",
        "  y_predict_NB = nb.predict(X_test)\n",
        "  # print(y_predict_NB)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_NB,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['NaiveBayes'] = evaluateBinaryClassification(y_predict_NB,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['Naive Bayes'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "ws_KC2tLw0NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine"
      ],
      "metadata": {
        "id": "9KkDmXgYxcBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  model = svm.SVC()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_predict_SVM = model.predict(X_test)\n",
        "  # print(y_predict_SVM)\n",
        "  \n",
        "  list = evaluateBinaryClassification(y_predict_SVM,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['SVM'] = evaluateBinaryClassification(y_predict_SVM,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['SVM'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "9BD2scC_xU7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "9wVoI97zyAdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "  RandomForest = RandomForestClassifier()\n",
        "  RandomForest.fit(X_train,y_train)\n",
        "\n",
        "  y_predict_RR = RandomForest.predict(X_test)\n",
        "  # print(y_predict_RR)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_RR,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['RandomForest'] = evaluateBinaryClassification(y_predict_RR,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "  # print(\"<___________________________>\")\n",
        "  # print(RandomForest.oob_score_)\n",
        "\n",
        "  # print(\"<___________________________>\")\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['Random Forest'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "mFDwxGlnx1iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Discriminant Analysis"
      ],
      "metadata": {
        "id": "aAs_jVDXy005"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  lda = LinearDiscriminantAnalysis()          #not working for n_components=2\n",
        "  X_train = lda.fit_transform(X_train, y_train)\n",
        "  X_test = lda.transform(X_test)\n",
        "\n",
        "  classifier = RandomForestClassifier()\n",
        "  classifier.fit(X_train, y_train)\n",
        "\n",
        "  #plotDecisionTree(classTree, feature_names=X_train.columns, class_names=classTree.classes_)\n",
        "  y_predict_LDA = classifier.predict(X_test)\n",
        "  # print(y_predict_LDA)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_LDA,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['LDA'] = evaluateBinaryClassification(y_predict_LDA,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['LDA'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "uHn3n0uJyLeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QDA Extra Trees Classifier"
      ],
      "metadata": {
        "id": "9qJnjjFr0Ie1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "X = data.iloc[:,:-1]\n",
        "Y = data.iloc[:,-1]\n",
        "\n",
        "# Building the model\n",
        "extra_tree_forest = ExtraTreesClassifier()\n",
        "  \n",
        "# Training the model\n",
        "extra_tree_forest.fit(X, Y)\n",
        "  \n",
        "# Computing the importance of each feature\n",
        "feature_importance = extra_tree_forest.feature_importances_\n",
        "  \n",
        "# Normalizing the individual importances\n",
        "feature_importance_normalized = np.std([tree.feature_importances_ for tree in extra_tree_forest.estimators_],axis = 0)\n",
        "\n",
        "# Plotting a Bar Graph to compare the models\n",
        "plt.bar(X.columns, feature_importance_normalized)\n",
        "plt.tick_params(axis='x', which='major', labelsize=6)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.xlabel('Feature Labels')\n",
        "plt.ylabel('Feature Importances')\n",
        "plt.title('Comparison of different Feature Importances')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wiG3ALg1z_sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "lSqc97bl0cV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  model = GradientBoostingClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_predict_GradientBoostingClassifier = model.predict(X_test)\n",
        "  # print(y_predict_GradientBoostingClassifier)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_GradientBoostingClassifier,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['GradientBoostingClassifier'] = evaluateBinaryClassification(y_predict_GradientBoostingClassifier,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['Gradient Boosting'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "3fchE1ET0Yy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LGMB Classifier"
      ],
      "metadata": {
        "id": "aJ2FI1y10uCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  model = LGBMClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_predict_LGMBclassifier = model.predict(X_test)\n",
        "  # print(y_predict_LGMBclassifier)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_LGMBclassifier,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  # compare_df.loc['LGBMclassifier'] = evaluateBinaryClassification(y_predict_LGMBclassifier,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['LGBMclassifier'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "DK3OA7P50q2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XgBoost Classifier"
      ],
      "metadata": {
        "id": "7WMRafoV0_K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT WORKING\n",
        "\n",
        "# kfold = KFold(n_splits=10, shuffle=True)\n",
        "# avg_accuracy = 0\n",
        "# avg_recall = 0\n",
        "# avg_precision = 0\n",
        "# avg_fscore = 0\n",
        "# max_accuracy = 0;\n",
        "# for train_index,test_index in kfold.split(data):\n",
        "\n",
        "#                # Get the train-test subsets\n",
        "#   X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "#   y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "#         # Scale the X features according to training data \n",
        "#   scaler = StandardScaler()\n",
        "#   X_train = scaler.fit_transform(X_train)\n",
        "#   X_test = scaler.transform(X_test)\n",
        "\n",
        "#   model = XGBClassifier()\n",
        "#   model.fit(X_train, y_train)\n",
        "#   y_predict_XgBoost = model.predict(X_test)\n",
        "#   # print(y_predict_XgBoost)\n",
        "\n",
        "#   list = evaluateBinaryClassification(y_predict_XgBoost,y_test)\n",
        "#   max_accuracy = max(max_accuracy,list[0])\n",
        "#   avg_accuracy += list[0]\n",
        "#   avg_recall += list[1]\n",
        "#   avg_precision += list[2]\n",
        "#   avg_fscore += list[3]\n",
        "\n",
        "#   # compare_df.loc['XgBoost'] = evaluateBinaryClassification(y_predict_XgBoost,y_test)\n",
        "#   # print(compare_df)\n",
        "\n",
        "# avg_accuracy /= kfold.get_n_splits()\n",
        "# avg_recall /= kfold.get_n_splits()\n",
        "# avg_precision /= kfold.get_n_splits()\n",
        "# avg_fscore /= kfold.get_n_splits()\n",
        "# compare_df.loc['XgBoost'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# # print(compare_df)"
      ],
      "metadata": {
        "id": "RWjVaUOH075w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Classifier"
      ],
      "metadata": {
        "id": "5pxfvVsq1PZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "avg_accuracy = 0\n",
        "avg_recall = 0\n",
        "avg_precision = 0\n",
        "avg_fscore = 0\n",
        "max_accuracy = 0;\n",
        "for train_index,test_index in kfold.split(data):\n",
        "\n",
        "               # Get the train-test subsets\n",
        "  X_train, X_test = data.iloc[train_index,:-1], data.iloc[test_index,:-1]\n",
        "  y_train, y_test = data.iloc[train_index,-1], data.iloc[test_index,-1]\n",
        "\n",
        "        # Scale the X features according to training data \n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "  mlp = MLPClassifier()\n",
        "  mlp.fit(X_train, y_train)\n",
        "  y_predict_mlp = mlp.predict(X_test)\n",
        "\n",
        "  list = evaluateBinaryClassification(y_predict_mlp,y_test)\n",
        "  max_accuracy = max(max_accuracy,list[0])\n",
        "  avg_accuracy += list[0]\n",
        "  avg_recall += list[1]\n",
        "  avg_precision += list[2]\n",
        "  avg_fscore += list[3]\n",
        "\n",
        "  #compare_df = pd.DataFrame()\n",
        "  # compare_df.loc['MLP'] = evaluateBinaryClassification(y_predict_mlp,y_test)\n",
        "  # print(compare_df)\n",
        "\n",
        "avg_accuracy /= kfold.get_n_splits()\n",
        "avg_recall /= kfold.get_n_splits()\n",
        "avg_precision /= kfold.get_n_splits()\n",
        "avg_fscore /= kfold.get_n_splits()\n",
        "compare_df.loc['MLP'] = [max_accuracy,avg_accuracy,avg_recall,avg_precision,avg_fscore]\n",
        "# print(compare_df)"
      ],
      "metadata": {
        "id": "oljFojqR1KYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(compare_df)"
      ],
      "metadata": {
        "id": "tGjmGjEt1OjR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}