{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('opinion_lexicon') #Download Opinion Dictionary\n",
    "from nltk.corpus import opinion_lexicon\n",
    "positive_wds = set(opinion_lexicon.positive())\n",
    "negative_wds = set(opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling different fragments of tweets\n",
    "\n",
    "data1=pd.read_csv(\"/2018/jan_1-14_2018.csv\")\n",
    "data2=pd.read_csv(\"/2018/jan_15_2018.csv\")\n",
    "data3=pd.read_csv(\"/2018/feb_march_2018.csv\")\n",
    "data4=pd.read_csv(\"/2018/tweets_april_may_2018.csv\")\n",
    "data5=pd.read_csv(\"/2018/june_2018.csv\")\n",
    "data6=pd.read_csv(\"/2018/july_2018.csv\")\n",
    "data7=pd.read_csv(\"/2018/aug_sep_2018.csv\")\n",
    "data8=pd.read_csv(\"/2018/oct_2018.csv\")\n",
    "data9=pd.read_csv(\"/2018/nov_2018.csv\")\n",
    "data10=pd.read_csv(\"/2018/dec_2018.csv\")\n",
    "\n",
    "data = pd.concat([data1,data2,data3,data4,data5,data6,data7,data8,data9,data10], axis=0)\n",
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to calculate the NLTK sentiments \n",
    "def score_sent(sent):\n",
    "    \"\"\"Returns a score btw -1 and 1\"\"\"\n",
    "    sent = [e.lower() for e in sent if e.isalnum()]\n",
    "    total = len(sent)\n",
    "    pos = len([e for e in sent if e in positive_wds])\n",
    "    neg = len([e for e in sent if e in negative_wds])\n",
    "    if total > 0:\n",
    "        return (pos - neg) / total\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def score_review(s):\n",
    "    sentiment_scores = []\n",
    "    sents = sent_tokenize(str(s))\n",
    "    for sent in sents:\n",
    "        wds = word_tokenize(sent)\n",
    "        sent_scores = score_sent(wds)\n",
    "        sentiment_scores.append(sent_scores)\n",
    "    return sum(sentiment_scores) / len(sentiment_scores)\n",
    "\n",
    "def score_to_rating(value):\n",
    "    if value > 0.2:\n",
    "        return 2\n",
    "    if value <= 0.2 and value >= -0.2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Calculating the NLTK sentiments for each tweet\n",
    "data['Sentiment'] = data['Tweet'].apply(score_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[]\n",
    "for l in data[\"Date\"]:\n",
    "    dates.append(datetime.strftime(parse(l),'%Y/%m/%d'))\n",
    "\n",
    "#Changing the dates to only have the date; Eliminating the time section     \n",
    "data[\"Date\"]=dates\n",
    "\n",
    "#Grouping all tweets for a day\n",
    "datewise_grps = data.groupby(data[\"Date\"])\n",
    "\n",
    "#Number of tweets for the day\n",
    "num_tweets = datewise_grps.size()\n",
    "\n",
    "#Average value for the day\n",
    "downsampled_data=datewise_grps.mean()\n",
    "\n",
    "#Adding the count attribute\n",
    "downsampled_data[\"Tweets_count\"]=num_tweets\n",
    "\n",
    "#saving the files\n",
    "downsampled_data.to_csv(\"filename.csv\")\n",
    "\n",
    "#The same process was applied to the news headlines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a390a84a9b37e0e690eddf20c3b7c5c68f879ad5e9909cce4d96802a8f5875ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
